{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massively Parallel Processing (MPP) Systems for Cloud Based Big Data processing\n",
    "\n",
    "# Apache Spark, the Industry Standard for MPP Systems\n",
    "\n",
    "# Machine Learning using Spark\n",
    "\n",
    "   * ## Lightning-fast unified analytics engine\n",
    "   * ## Run workloads 100x faster\n",
    "   * ## Write applications quickly in Java, Scala, Python, R, and SQL\n",
    "   * ## Combine SQL, streaming, and complex analytics\n",
    "   * ## Runs everywhere such as \n",
    "           * Hadoop, \n",
    "           * Apache \n",
    "           * Mesos, \n",
    "           * Kubernetes, \n",
    "           * Standalone, or in the cloud. \n",
    "           * It can access diverse data sources\n",
    "           \n",
    " https://www.youtube.com/watch?v=TgiBvKcGL24\n",
    " \n",
    " https://www.youtube.com/watch?v=qKYpMPPL-fo\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "x = 8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n"
     ]
    }
   ],
   "source": [
    "y\n",
    "num_samples = 100000000\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n"
     ]
    }
   ],
   "source": [
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8280c3a3990e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Pi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000\n",
    "def inside(p):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " contentRDD =sc.textFile('file///C:/users/utpal/desktop/ForMorgan.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonempty_lines = contentRDD.filter(lambda x: len(x) > 0)\n",
    "print(nonempty_lines.take(5))\n",
    "print('-------------------')\n",
    "index = 0\n",
    "for line in nonempty_lines.take(5):\n",
    "    print('Line-', index, line)\n",
    "    print('-------------------')\n",
    "    index += 1\n",
    "words = nonempty_lines.flatMap(lambda x: x.split(' '))\n",
    "wordcount = words.map(lambda x:(x,1)) .reduceByKey(lambda x,y: x+y) .map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "print('====================================')\n",
    "for word in wordcount.collect():\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.load('file:///C:/users/utpal/desktop/regr_data_set/pima-indians-diabetes.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(max(\"Pregnancies\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Pregnancies\", \"Age\", \"Outcome\"). show(10, truncate=False)\n",
    "df.select(\"*\").show(10, truncate=False)\n",
    "df.select(\"*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select(\"Insulin\").distinct()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasdf.count()\n",
    "pandasdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterest = sqlContext.read.load('file:///C:/users/utpal/desktop/regr_data_set/InterestRate.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterest.select(\"Interest_Rate\").distinct().show()\n",
    "dfInterest.select(\"Interest_Rate\").distinct().count()\n",
    "dfInterest.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterest = sqlContext.read.load('file:///c:/users/utpal/desktop/otherdata/train_data.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInterest.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "plan_indexer = StringIndexer(inputCol = 'Property_Area', outputCol = 'Prop_Area')\n",
    "labeller = plan_indexer.fit(dfInterest)\n",
    "\n",
    "dfI = labeller.transform(dfInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfI.select(\"Property_Area\", \"Prop_Area\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfI.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = sqlContext.read.load('file:///C:/users/utpal/desktop/pyspark/pa_reimb_county_2014.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(20)\n",
    "\n",
    "newdf_pandas = newdf.toPandas()\n",
    "newdf_pandas.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.select(\"*\").show(10, truncate=False)\n",
    "newdf.select(\"*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MyStatsProblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d9e85b1e2f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mMyStatsProblib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmystats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mINPUT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"file///C:/Users/utpal/desktop/pyspark/pa_reimb_county_2014.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'MyStatsProblib'"
     ]
    }
   ],
   "source": [
    "# sc = pyspark.SparkContext(appName=\"wc\")\n",
    "# contentRDD =sc.textFile('file///C:/users/utpal/desktop/ForMorgan.txt')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import MyStatsProblib as mystats\n",
    "\n",
    "INPUT = \"file///C:/Users/utpal/desktop/pyspark/pa_reimb_county_2014.csv\"\n",
    "\n",
    "df_sql = sqlContext.read.load('file:///C:/users/utpal/desktop/pyspark/pa_reimb_county_2014-1.csv', \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='true', \n",
    "                          inferSchema='true')\n",
    "# df_sql.count()\n",
    "df_pandas = df_sql.toPandas()\n",
    "\n",
    "#df_pandas.isnull().sum()\n",
    "df_pandas[\"Total Medicare reimbursements per enrollee (Parts A and B) (2014)\"].fillna(0,inplace=True)\n",
    "\n",
    "col_4 = df_pandas[\"Total Medicare reimbursements per enrollee (Parts A and B) (2014)\"]\n",
    "\n",
    "mean = mystats.mean(col_4)\n",
    "stddev = mystats.standard_deviation(col_4)\n",
    "\n",
    "print('Mean =', mean)\n",
    "print('Stddev = ', stddev)\n",
    "lower_limit = (mean - 3 * stddev)\n",
    "upper_limit = (mean + 4 * stddev)\n",
    "upper_limit1 = (mean + 3 * stddev)\n",
    "print('Lower Limit = ', lower_limit, 'Upper Limit = ', upper_limit)\n",
    "\n",
    "#outliers = df_pandas.loc[(col_4 < lower_limit) | (col_4 > upper_limit)]\n",
    "outliers = df_pandas.loc[(col_4 > upper_limit)]\n",
    "outliers1 = df_pandas.loc[(col_4 > upper_limit1)]\n",
    "outliers.head(10)\n",
    "outliers.count()\n",
    "# print(outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt1\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "df_sql = df_sql.na.drop()\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Total Medicare reimbursements per enrollee (Parts A and B) (2014)\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df_sql)\n",
    "\n",
    "kmeans = KMeans(k=10, seed=1)  # 5 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))\n",
    "\n",
    "transformed = model.transform(new_df)\n",
    "\n",
    "df_pandas1 = transformed.toPandas()\n",
    "\n",
    "df_pandas1['prediction']\n",
    "\n",
    "df_selected = df_pandas1[['County ID', 'County name', 'Total Medicare reimbursements per enrollee (Parts A and B) (2014)','prediction']]\n",
    "df_selected\n",
    "df_head = df_selected.head(500)\n",
    "\n",
    "plt1.scatter(df_head['County ID'],  df_head['Total Medicare reimbursements per enrollee (Parts A and B) (2014)'], df_head['prediction'], label='True Position', cmap='rainbow')\n",
    "plt1.show()\n",
    "\n",
    "plt1.scatter(df_pandas1['County ID'],  df_pandas1['Total Medicare reimbursements per enrollee (Parts A and B) (2014)'], df_pandas1['prediction'], label='True Position', cmap='rainbow')\n",
    "plt1.show()\n",
    "\n",
    "plt1.scatter(outliers['Total Medicare reimbursements per enrollee (Parts A and B) (2014)'], outliers['County name'], marker='*', color='red', s=1000, label='True Position', cmap='rainbow')\n",
    "plt1.show()\n",
    "\n",
    "plt1.scatter(outliers1['Total Medicare reimbursements per enrollee (Parts A and B) (2014)'], outliers1['County name'], marker='*', color='red', s=200, label='True Position', cmap='rainbow')\n",
    "plt1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
