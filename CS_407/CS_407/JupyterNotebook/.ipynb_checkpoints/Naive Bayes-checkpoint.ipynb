{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability, Conditional Probability, Bayes' Theorem\n",
    "## Conditional Probability\n",
    "**Conditional Probability is defined as the probability of an event ( A ), given that another ( B ) has already occurred.**\n",
    "\n",
    "If events A and B are not independent, then the probability of the intersection of A and B (the probability that both P(B|A) = vents occur) is defined by \n",
    "P(A and B) = P(A)P(B|A).\n",
    "\n",
    "From this definition, the conditional probability P(B|A) is easily obtained by dividing by P(A):\n",
    "\n",
    "**P(B|A) = P(B and A) / P(A)**\n",
    "\n",
    "In the Predictive Analytics section we will learn a very widely used **Classification** algorithm called the **Naive Bayes Classifiaction Algorithm**.\n",
    "\n",
    "It is a Machine Learning algorithm that is often used in data sets with multiple attributes. It is very easy to calculate and hence is often used to classify things in real time, such as \"if an email containing a set of key words is classified as spam\", \"a newly published article belongs to a class of articles\", \"if an insurance claim, just submitted is real or fraud\" etc.\n",
    "\n",
    "The **Bayes** part of the name comes from Thomas Bayes, the inventor of the foundational Bayes' theorem and the **Naive** part of the name comes from the assumption that the factors guiding the occurrance of an event are **independent** of each other, even though in real life, they may not be so (a somewhat **naive** assumption). However, this algorithm produces very good/reliable results and is widely used.\n",
    "\n",
    "\n",
    "\n",
    "## Bayes' Theorem\n",
    "Bayes' Theorem (also called Bayes' Law or Bayes' Formula) is stated as\n",
    "\n",
    "***Probability of an event A given that an event B has occurred, is equal to the probability of B given A has occurred multiplied by the probability of A given B has occurred divided by the probability of B***\n",
    "\n",
    "***P(A|B) = (P(B|A) X P(A))/P(B)***\n",
    "\n",
    "where\n",
    "\n",
    "P(A|B) = Probability of event A given the event B has occurred\n",
    "\n",
    "P(B|A) = Probability of event B given the event A has occurred\n",
    "\n",
    "P(A), P(B) = Probabilities of event A and B respectively\n",
    "\n",
    "### Commonly used terms in Bayesian Classification\n",
    "A is called the **Proposition** and B is called the **Evidence**\n",
    "\n",
    "P(A) is called the **Prior Probability of Proposition** and P(B) is called the **Prior probability of Evidence**\n",
    "\n",
    "P(A|B) is called the **Posterior**\n",
    "\n",
    "P(B|A) is called the **Likelyhood**\n",
    "\n",
    "\n",
    "In other words\n",
    "\n",
    "***Posterior = (Likelihood X Prior Probability of Proposition)/Prior Probability of Evidence***\n",
    "\n",
    "### Bayesian Theorem as applied to Naive Bayes Algorithm\n",
    "In Machine Learning classification there are multiple clesses C1, C2, C3...and each class with multiple features x1, x2, x3...(e.g. an insurance claim is in class 'Valid' or 'Fraud' and each claim has features such as 'amount of claim', 'doctor submitting the claim', 'amount of the claim', 'frequency of high value claim for same treatment by the same doctor' etc.). The aim of the algorithm is to determine the **Conditional Probability** of an object (an insurance claim) with features x1, x2,...xn belonging to a class Ci.\n",
    "\n",
    "We will learn Bayesin Classification and it's calculation (using Python) in much more details in the **Predictive Analytics** section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=sjUDlJfdnKM.\n",
    "\n",
    "https://www.youtube.com/watch?v=CPqOCI0ahss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mth\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing K_Mean Clustering, Naive Bayes and Logistic Regression using Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris.feature_names\n",
    "iris.data\n",
    "iris.target_names\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sepal Length  Sepal Width  Petal Length  Petal Width\n",
       "0            5.1          3.5           1.4          0.2\n",
       "1            4.9          3.0           1.4          0.2\n",
       "2            4.7          3.2           1.3          0.2\n",
       "3            4.6          3.1           1.5          0.2\n",
       "4            5.0          3.6           1.4          0.2\n",
       "5            5.4          3.9           1.7          0.4\n",
       "6            4.6          3.4           1.4          0.3\n",
       "7            5.0          3.4           1.5          0.2\n",
       "8            4.4          2.9           1.4          0.2\n",
       "9            4.9          3.1           1.5          0.1\n",
       "10           5.4          3.7           1.5          0.2\n",
       "11           4.8          3.4           1.6          0.2\n",
       "12           4.8          3.0           1.4          0.1\n",
       "13           4.3          3.0           1.1          0.1\n",
       "14           5.8          4.0           1.2          0.2\n",
       "15           5.7          4.4           1.5          0.4\n",
       "16           5.4          3.9           1.3          0.4\n",
       "17           5.1          3.5           1.4          0.3\n",
       "18           5.7          3.8           1.7          0.3\n",
       "19           5.1          3.8           1.5          0.3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(iris.data)\n",
    "iris_df.columns = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "iris_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0\n",
       "5  0\n",
       "6  0\n",
       "7  0\n",
       "8  0\n",
       "9  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "140  2\n",
       "141  2\n",
       "142  2\n",
       "143  2\n",
       "144  2\n",
       "145  2\n",
       "146  2\n",
       "147  2\n",
       "148  2\n",
       "149  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "30   0\n",
       "31   0\n",
       "32   0\n",
       "33   0\n",
       "34   0\n",
       "..  ..\n",
       "100  2\n",
       "101  2\n",
       "102  2\n",
       "103  2\n",
       "104  2\n",
       "\n",
       "[75 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df1 = pd.DataFrame(iris.target)\n",
    "iris_df1.head(10)\n",
    "iris_df1.tail(10)\n",
    "iris_df1[30:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using K-Mean Clustering\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n",
      " 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n",
      " 0 2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        50\n",
      "           1       0.00      0.00      0.00        50\n",
      "           2       0.23      0.28      0.25        50\n",
      "\n",
      "    accuracy                           0.09       150\n",
      "   macro avg       0.08      0.09      0.08       150\n",
      "weighted avg       0.08      0.09      0.08       150\n",
      "\n",
      "[[ 0 50  0]\n",
      " [ 2  0 48]\n",
      " [36  0 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmodel = kmeans.fit(iris.data)\n",
    "expected = iris.target\n",
    "#predicted = kmodel.labels_\n",
    "predicted = kmodel.predict(iris.data)\n",
    "print('======>', predicted)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(expected, kmodel.labels_))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Gaussian Naive Bayes Model\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_target = pd.DataFrame(iris.target)\n",
    "iris_target.columns = ['Target Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbmodel = GaussianNB()\n",
    "nbmodel.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = iris.target\n",
    "predicted = nbmodel.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.94      0.94      0.94        50\n",
      "           2       0.94      0.94      0.94        50\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using  Multi-factors Logistic Regression\n",
    "\n",
    "## Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "#print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Result ==================\n",
      "++++++++++++++++ Actual/Expected ++++++++++++++++++\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "++++++++++++++ Predicted ++++++++++++++++++++++++++\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "=============== Model Performance Results ===========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.98      0.94      0.96        50\n",
      "           2       0.94      0.98      0.96        50\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n",
      "============ Model Confusion Matrix ===========\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  1 49]]\n",
      "=============== Model Accuracy ==============\n",
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(iris.data, iris.target)\n",
    "#print(lr_model)\n",
    "# make predictions\n",
    "expected = iris.target\n",
    "predicted = lr_model.predict(iris.data)\n",
    "\n",
    "print('================ Result ==================')\n",
    "print('++++++++++++++++ Actual/Expected ++++++++++++++++++')\n",
    "print(expected)\n",
    "print('++++++++++++++ Predicted ++++++++++++++++++++++++++')\n",
    "print(predicted)\n",
    "# summarize the fit of the model\n",
    "print('=============== Model Performance Results ===========')\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print('============ Model Confusion Matrix ===========')\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print('=============== Model Accuracy ==============')\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Naive Bayes and Multi-Class Regression classifiers Native Indian Diabetes data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintions of \"Accuracy\", \"Precision\" and \"Recall\" scores of the prediction of a model\n",
    "\n",
    "### Accuracy = Number of correct prediction / Total Number of Predition\n",
    "\n",
    "### Precision = True Positive / (True Positive + False Positive)\n",
    "\n",
    "### Recall = True Positive / (True Positive + False Negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "desktop = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')\n",
    "mylines = []\n",
    "path = desktop + \"\\csvFilePath.txt\"\n",
    "with open (path, 'rt') as myfile:\n",
    "    for myline in myfile:                \n",
    "        mylines.append(myline)\n",
    "New_path = mylines[6]\n",
    "df0 = pd.read_csv(New_path[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodP  SkinThick  Insulin   BMI  \\\n",
       "0             6      148      72         35        0  33.6   \n",
       "1             1       85      66         29        0  26.6   \n",
       "2             8      183      64          0        0  23.3   \n",
       "3             1       89      66         23       94  28.1   \n",
       "4             0      137      40         35      168  43.1   \n",
       "5             5      116      74          0        0  25.6   \n",
       "6             3       78      50         32       88  31.0   \n",
       "7            10      115       0          0        0  35.3   \n",
       "8             2      197      70         45      543  30.5   \n",
       "9             8      125      96          0        0   0.0   \n",
       "10            4      110      92          0        0  37.6   \n",
       "11           10      168      74          0        0  38.0   \n",
       "12           10      139      80          0        0  27.1   \n",
       "13            1      189      60         23      846  30.1   \n",
       "14            5      166      72         19      175  25.8   \n",
       "15            7      100       0          0        0  30.0   \n",
       "16            0      118      84         47      230  45.8   \n",
       "17            7      107      74          0        0  29.6   \n",
       "18            1      103      30         38       83  43.3   \n",
       "19            1      115      70         30       96  34.6   \n",
       "\n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                      0.627   50        1  \n",
       "1                      0.351   31        0  \n",
       "2                      0.672   32        1  \n",
       "3                      0.167   21        0  \n",
       "4                      2.288   33        1  \n",
       "5                      0.201   30        0  \n",
       "6                      0.248   26        1  \n",
       "7                      0.134   29        0  \n",
       "8                      0.158   53        1  \n",
       "9                      0.232   54        1  \n",
       "10                     0.191   30        0  \n",
       "11                     0.537   34        1  \n",
       "12                     1.441   57        0  \n",
       "13                     0.398   59        1  \n",
       "14                     0.587   51        1  \n",
       "15                     0.484   32        1  \n",
       "16                     0.551   31        1  \n",
       "17                     0.254   31        1  \n",
       "18                     0.183   33        0  \n",
       "19                     0.529   32        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose      BloodP   SkinThick     Insulin  \\\n",
       "count   768.000000  768.000000  768.000000  768.000000  768.000000   \n",
       "mean      3.845052  120.894531   69.105469   20.536458   79.799479   \n",
       "std       3.369578   31.972618   19.355807   15.952218  115.244002   \n",
       "min       0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%       1.000000   99.000000   62.000000    0.000000    0.000000   \n",
       "50%       3.000000  117.000000   72.000000   23.000000   30.500000   \n",
       "75%       6.000000  140.250000   80.000000   32.000000  127.250000   \n",
       "max      17.000000  199.000000  122.000000   99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as gnb\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]=15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodP</th>\n",
       "      <th>SkinThick</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodP  SkinThick  Insulin   BMI  \\\n",
       "0             6      148      72         35        0  33.6   \n",
       "1             1       85      66         29        0  26.6   \n",
       "2             8      183      64          0        0  23.3   \n",
       "3             1       89      66         23       94  28.1   \n",
       "4             0      137      40         35      168  43.1   \n",
       "5             5      116      74          0        0  25.6   \n",
       "6             3       78      50         32       88  31.0   \n",
       "8             2      197      70         45      543  30.5   \n",
       "10            4      110      92          0        0  37.6   \n",
       "11           10      168      74          0        0  38.0   \n",
       "12           10      139      80          0        0  27.1   \n",
       "13            1      189      60         23      846  30.1   \n",
       "14            5      166      72         19      175  25.8   \n",
       "16            0      118      84         47      230  45.8   \n",
       "17            7      107      74          0        0  29.6   \n",
       "18            1      103      30         38       83  43.3   \n",
       "19            1      115      70         30       96  34.6   \n",
       "20            3      126      88         41      235  39.3   \n",
       "21            8       99      84          0        0  35.4   \n",
       "22            7      196      90          0        0  39.8   \n",
       "\n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                      0.627   50        1  \n",
       "1                      0.351   31        0  \n",
       "2                      0.672   32        1  \n",
       "3                      0.167   21        0  \n",
       "4                      2.288   33        1  \n",
       "5                      0.201   30        0  \n",
       "6                      0.248   26        1  \n",
       "8                      0.158   53        1  \n",
       "10                     0.191   30        0  \n",
       "11                     0.537   34        1  \n",
       "12                     1.441   57        0  \n",
       "13                     0.398   59        1  \n",
       "14                     0.587   51        1  \n",
       "16                     0.551   31        1  \n",
       "17                     0.254   31        1  \n",
       "18                     0.183   33        0  \n",
       "19                     0.529   32        1  \n",
       "20                     0.704   27        0  \n",
       "21                     0.388   50        0  \n",
       "22                     0.451   41        1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(724, 9)\n",
      "(543, 9)\n",
      "(181, 9)\n"
     ]
    }
   ],
   "source": [
    "data_mod = df0[(df0.BloodP != 0) & (df0.BMI != 0) & (df0.Glucose != 0)]\n",
    "data_mod.head(20)\n",
    "train, test = train_test_split(data_mod, test_size=0.25)\n",
    "print(data_mod.shape)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes  mean accuracy:  73.666 % std:  0.262 %\n",
      "Logistic Regression  mean accuracy:  76.242 % std:  0.08 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "features = ['Pregnancies', 'Glucose', 'BloodP', 'SkinThick', 'BMI', 'Age', 'Insulin', 'DiabetesPedigreeFunction']\n",
    "target = 'Outcome'\n",
    "\n",
    "classifiers = [gnb(), lr()]\n",
    "\n",
    "classifier_names = ['Gaussian Naive Bayes', 'Logistic Regression']\n",
    "\n",
    "for clf, clf_name in zip(classifiers, classifier_names):\n",
    "    cv_scores = cross_val_score(clf, train[features], train[target], cv=5)\n",
    "    \n",
    "    print(clf_name, ' mean accuracy: ', round(cv_scores.mean()*100, 3), '% std: ', round(cv_scores.var()*100, 3),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the X and Y axes of the Heat Map\n",
    "\n",
    "### X-Axis = Prediction, 0 = True 1 = False\n",
    "### Y-Axis = Actual, 0 = True, 1 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for Gaussian naive bayes classifier: 78.45 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       122\n",
      "           1       0.71      0.58      0.64        59\n",
      "\n",
      "    accuracy                           0.78       181\n",
      "   macro avg       0.76      0.73      0.74       181\n",
      "weighted avg       0.78      0.78      0.78       181\n",
      "\n",
      "[[108  14]\n",
      " [ 25  34]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix for Gaussian naive bayes classifier')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c601c185b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model_gnb = gnb().fit(train[features], train[target])\n",
    "y_hat_gnb = final_model_gnb.predict(test[features])\n",
    "\n",
    "print('test accuracy for Gaussian naive bayes classifier:', \\\n",
    "      round(accuracy_score(test[target], y_hat_gnb)*100, 2),'%')\n",
    "print(metrics.classification_report(test[target], y_hat_gnb))\n",
    "print(confusion_matrix(test[target], y_hat_gnb))\n",
    "plt.title('confusion matrix for Gaussian naive bayes classifier')\n",
    "sns.heatmap(confusion_matrix(test[target], y_hat_gnb), annot=True, cmap=\"YlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy for Logistic Regression classifier: 73.48 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'confusion matrix for Logistic Regression classifier')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82       122\n",
      "           1       0.64      0.42      0.51        59\n",
      "\n",
      "    accuracy                           0.73       181\n",
      "   macro avg       0.70      0.65      0.66       181\n",
      "weighted avg       0.72      0.73      0.72       181\n",
      "\n",
      "[[108  14]\n",
      " [ 34  25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c601c18f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model_lr = lr().fit(train[features], train[target])\n",
    "y_hat_lr = final_model_lr.predict(test[features])\n",
    "\n",
    "print('test accuracy for Logistic Regression classifier:', \\\n",
    "      round(accuracy_score(test[target], y_hat_lr)*100, 2),'%')\n",
    "plt.title('confusion matrix for Logistic Regression classifier')\n",
    "print(metrics.classification_report(test[target], y_hat_lr))\n",
    "print(confusion_matrix(test[target], y_hat_lr))\n",
    "sns.heatmap(confusion_matrix(test[target], y_hat_lr), annot=True, cmap=\"YlGn\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
